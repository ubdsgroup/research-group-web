@InProceedings{Mahapatra2017,
  Title                    = {S-Isomap++: Multi Manifold Learning from Streaming Data},
  Author                   = {Suchismit Mahapatra and Varun Chandola},
  Booktitle                = {IEEE International Conference on Big Data},
  Year                     = {2017},
  Month                    = {Dec},
  abstract       = {Manifold learning based methods have been widely used for non-linear dimensionality reduction (NLDR). However, in many practical settings, the need to process streaming data is a challenge for such methods, owing to the high computational complexity involved. Moreover, most methods operate under the assumption that the input data is sampled from a single manifold, embedded in a high dimensional space. We propose a method for streaming NLDR when the observed data is either sampled from multiple manifolds or irregularly sampled from a single manifold. We show that existing NLDR methods, such as Isomap, fail in such situations, primarily because they rely on smoothness and continuity of the underlying manifold, which is violated in the scenarios explored in this paper. However, the proposed algorithm is able to learn effectively in presence of multiple, and potentially intersecting, manifolds, while allowing for the input data to arrive as a massive stream.},
  url       = {https://ieeexplore.ieee.org/document/8257987},
}
@INPROCEEDINGS{8048954, 
author={N. Sorkunlu and V. Chandola and A. Patra}, 
booktitle={2017 IEEE International Conference on Cluster Computing (CLUSTER)}, 
title={Tracking System Behavior from Resource Usage Data}, 
year={2017}, 
volume={}, 
number={}, 
pages={410-418}, 
keywords={data analysis;error statistics;parallel processing;statistical analysis;tensors;Lonestar4 system;TACC;Texas Advanced Computing Center;anomaly detection;error statistic;high performance computing system;low rank tensor decomposition;performance anomalies;resource usage data;resource utilization;scalar performance metric;system logs;tracking system behavior;usage metrics;Correlation;Error analysis;Measurement;Monitoring;Tensile stress;Tools;Feature Extraction;HPC;Tensor Analysis;anomaly detection;performance monitoring}, 
doi={10.1109/CLUSTER.2017.70}, 
ISSN={}, 
month={Sept},}
@inproceedings{Schoeneman2017,
  author = {Schoeneman, F. and Mahapatra, S. and Chandola, V. and
    Napp, N. and Zola, J.},
  title = {Error Metrics for Learning Reliable Manifolds from Streaming Data},
  booktitle = {Proceedings of SIAM Data Mining Conference},
  year = {2017},
  month =  {April},
  url = {http://epubs.siam.org/doi/10.1137/1.9781611974973.84},
  abstract = {Spectral dimensionality reduction is frequently used to identify low-dimensional structure in high-dimensional data. However, learning manifolds, especially from the streaming data, is computationally and memory expensive. In this paper, we argue that a stable manifold can be learned using only a fraction of the stream, and the remaining stream can be mapped to the manifold in a significantly less costly manner. Identifying the transition point at which the manifold is stable is the key step. We present error metrics that allow us to identify the transition point for a given stream by quantitatively assessing the quality of a manifold learned using Isomap. We further propose an efficient mapping algorithm, called S-Isomap, that can be used to map new samples onto the stable manifold. We describe experiments on a variety of data sets that show that the proposed approach is computationally efficient without sacrificing accuracy.},
}
@InProceedings{Mahapatra2015,
  Title                    = {Modeling Graphs Using a Mixture of Kronecker Models},
  Author                   = {Suchismit Mahapatra and Varun Chandola},
  Booktitle                = {IEEE International Conference on Big Data},
  Year                     = {2015},
  Month                    = {Nov},
  abstract		   = {Generative models for graphs are increasingly becoming a popular tool for researchers to generate realistic approximations of graphs. While in the past, focus was on generating graphs which follow general laws, such as the power law for degree distribution, current models have the ability to learn from observed graphs and generate synthetic approximations. The primary emphasis of existing models has been to closely match different properties of a single observed graph. Such models, though stochastic, tend to generate samples which do not have significant variance in terms of the various graph properties. We argue that in many cases real graphs are sampled drawn from a graph population (e.g., networks sampled at various time points, social networks for individual schools, healthcare networks for different geographic regions, etc.). Such populations typically exhibit significant variance. However, existing models are not designed to model this variance, which could lead to issues such as overfitting. We propose a graph generative model that focuses on matching the properties of real graphs and the natural variance expected for the corresponding population. The proposed model adopts a mixture-model strategy to expand the expressiveness of Kronecker product based graph models (KPGM), while building upon the two strengths of KPGM, viz., ability to model several key properties of graphs and to scale to massive graph sizes using its elegant fractal growth based formulation. The proposed model, called x-Kronecker Product Graph Model, or xKPGM, allows scalable learning from observed graphs and generates samples that match the mean and variance of several salient graph properties. We experimentally demonstrate the capability of the proposed model to capture the inherent variability in real world graphs on a variety of publicly available graph data sets.},
  url			  = {http://ieeexplore.ieee.org/document/7363817},
}
@InProceedings{Jiang2015,
  author =   {Jialiang Jiang and Jessica Castner and Sharon Hewner and Varun Chandola},
  title =    {Improving Quality of Care using Data Science Driven Methods},
  booktitle = {{UNYTE} Scientific Session - Hitting the Accelerator: Health Research Innovation through Data Science },
  year =      {2015},
  month = {June},
}
@inproceedings{Kul2016,
  author = {Kul, Gokhan and Luong, Duc and Xie, Ting and Coonan, Patrick and Chandola, Varun and Kennedy, Oliver and Upadhyaya, Shambhu},
  title = {Ettu: Analyzing Query Intents in Corporate Databases},
  booktitle = {Proceedings of the 25th International Conference Companion on World Wide Web},
  year = {2016},
  pages = {463--466},
  abstract= {Insider threats to databases in the financial sector have become a very serious and pervasive security problem. This paper proposes a framework to analyze access patterns to databases by clustering SQL queries issued to the database. Our system Ettu works by grouping queries with other similarly structured queries. The small number of intent groups that result can then be efficiently labeled by human operators. We show how our system is designed and how the components of the system work. Our preliminary results show that our system accurately models user intent.},
  url = {http://dl.acm.org/citation.cfm?id=2888608}
} 

@InProceedings{Valecha2015,
  title = {A big data approach to rumor mitigation in Twitter microblog: A case of Boston bombings},
  booktitle = {WEB: The Thirteenth Workshop on e-Business},
  author = {R. Valecha and A. Sultania and V. Chandola and M. Agrawal and H. R. Rao.},
  year = {2015},
}

@article{Luong2016,
  author={D. T. A. Luong and D. Tran and V. Chandola and C. Fox and W. Pace and J. Vassalotti and J. Carroll and M. Dickinson and M. Withiam-Leitch and N. Satchindanand and M. Yang and C. Smail},
  title = {Extracting deep phenotypes for chronic kidney disease using electronic health records},
  journal = {eGems - Generating Evidence \& Methods to Improve Patient Outcomes (To Appear)}, 
  year = {2017},
}
@Inproceedings{Yang2015,
author="Yang, Zhi
and Chandola, Varun",
editor="Nalpantidis, Lazaros
and Kr{\"u}ger, Volker
and Eklundh, Jan-Olof
and Gasteratos, Antonios",
title="Surface Reconstruction from Intensity Image Using Illumination Model Based Morphable Modeling",
booktitle="Computer Vision Systems: 10th International Conference, ICVS 2015, Copenhagen, Denmark, July 6-9, 2015, Proceedings",
year="2015",
publisher="Springer International Publishing",
pages="117--127",
abstract="We present a new method for reconstructing depth of a known object from a single still image using deformed underneath sign matrix of a similar object. Existing Shape from Shading(SFS) methods try to establish a relationship between intensity values of a still image and surface normal of corresponding depth, but most of them resort to error minimization based approaches. Given the fact that these reconstruction approaches are fundamentally ill-posed, they have limited successes for surfaces like a human face. Photometric Stereo (PS) or Structure from Motion (SfM) based methods extend SFS by adding additional information/constraints about the target. Our goal is identical to SFS, however, we tackle the problem by building a relationship between gradient of depth and intensity value at the corresponding location of image of the same object. This formula is simplified and approximated for handing different materials, lighting conditions and, the underneath sign matrix is also obtained by resizing/deforming Region of Interest(ROI) with respect to its counterpart of a similar object. The target object is then reconstructed from its still image. In addition to the process, delicate details of the surface is also rebuilt using a Gabor Wavelet Network(GWN) on different ROIs. Finally, for merging the patches together, a Self-Organizing Maps(SOM) based method is used to retrieve and smooth boundary parts of ROIs. Compared with state of art SFS based methods, the proposed method yields promising results on both widely used benchmark datasets and images in the wild.",
url="http://dx.doi.org/10.1007/978-3-319-20904-3_11"
}
@inproceedings{DBLP:conf/sc/BaumanCPJ14,
  author    = {Paul T. Bauman and
    Varun Chandola and
      Abani K. Patra and
      Matthew D. Jones},
  title     = {Development of a computational and data-enabled science and engineering
    Ph.D. program},
  booktitle = {Proceedings of the Workshop on Education for High-Performance Computing,
    EduHPC '14, New Orleans, Louisiana, USA, November 16-21, 2014},
  pages     = {21--26},
  year      = {2014},
  abstract  = {The previous two decades have seen the successful deployment of Computational Science programs in universities across the globe. These programs are aimed at training scientists and engineers to tackle problems requiring interdisciplinary approaches to finding solutions to scientific and engineering problems and the development of new computing, as exemplified by the co-design approach to exascale architectures and applications. Thus, the programs emphasize preparation in applied mathematics, numerical analysis, and scientific computing in addition to science and engineering work relevant to the target application. The rise of so-called "Big-Data" applications and the use of large data in business decision support and even in computational science workflows like uncertainty analysis are driving a need for training in data sciences. This paper makes the argument that, rather than treating topics in machine learning, statistics, etc. as stand-alone fields of study that students learn as electives, data-science should be an integral part of interdisciplinary training for future researchers. This approach is at the core of the newly developed Computational and Data-Enabled Science and Engineering (CDSE) Ph.D. program at the University of Buffalo. This paper describes the development of the Ph.D. program, the target student audience, and strategies for effectively executing the proposed curriculum.},
  url       = {http://dx.doi.org/10.1109/EduHPC.2014.8},
}
@Inproceedings{Jiang2016,
  author    = {Jialiang Jiang and Sharon Hewner and Varun Chandola},
  year      = {2016},
  title     = {Hospital Readmission Prediction - Applying Hierarchical Sparsity Norms for Interpretable Models},
  booktitle = {Proceedings of the Machine Learning and Data Mining Conference (MLDM)},
  abstract  = {Hospital readmissions have become one of the key measures of healthcare quality. Preventable readmissions have been identified as one of the primary targets for reducing costs and improving healthcare delivery. However, most data driven studies for understanding readmissions have produced black box classification and predictive models with moderate performance, which precludes them from being used effectively within the decision support systems in the hospitals.
  In this paper we present an application of structured sparsity-inducing norms for predicting readmission risk for patients based on their disease history and demographics. Most existing studies have focused on hospital utilization, test results, etc., to assign a readmission label to each episode of hospitalization. However, we focus on assigning a readmission risk label to a patient based on her disease history. Our emphasis is on interpreting the models to improve the understanding of the readmission problem. To achieve this, we exploit the domain induced hierarchical structure available for the disease codes which are the features for the classification algorithm. We use a tree based sparsity-inducing regularization strategy that explicitly uses the domain hierarchy. The resulting model not only outperforms standard regularization procedures but is also highly sparse and interpretable. We analyze the model and identify several significant factors that have an effect on readmission risk. Some of these factors conform to existing beliefs, e.g., impact of surgical complications and infections during hospital stay. Other factors, such as the impact of mental disorder and substance abuse on readmission, provide empirical evidence for several pre-existing but unverified hypotheses. The analysis also reveals previously undiscovered connections such as the influence of socioeconomic factors like lack of housing and malnutrition.
  The findings of this study will be instrumental in designing the next generation decision support systems for preventing readmissions.},
}
